{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import csv\n",
    "# https://github.com/IShengFang/SpectralNormalizationKeras\n",
    "%run SpectralNormalizationKeras\n",
    "\n",
    "# Image related Libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
    "\n",
    "# Keras Libraries\n",
    "from keras.layers import Activation, BatchNormalization, Input, LeakyReLU, AveragePooling2D, Add, Concatenate, Flatten, Dense, Lambda, Reshape, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Custom Objects to load the model containing spectral normalized layers\n",
    "get_custom_objects().update({'ConvSN2D': ConvSN2D})\n",
    "get_custom_objects().update({'ConvSN2DTranspose': ConvSN2DTranspose})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The self attention layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, output_dim, dimension, **kwargs):\n",
    "        self.dimension = dimension\n",
    "        self.output_dim = output_dim\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "   \n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.gamma = self.add_weight(name='gamma', \n",
    "                                     shape=(1,),\n",
    "                                     initializer='zeros',\n",
    "                                     trainable=True)\n",
    "        self.f_ = ConvSN2D(self.output_dim//4, 1, strides=1, padding='same', name='selfAttentionFLayer')\n",
    "        self.g_ = ConvSN2D(self.output_dim//4, 1, strides=1, padding='same', name='selfAttentionGLayer')\n",
    "        self.h_ = ConvSN2D(self.output_dim//2, 1, strides=1, padding='same', name='selfAttentionHLayer')\n",
    "        self.v_ = ConvSN2D(self.output_dim, 1, strides=1, padding='same', name='selfAttentionVLayer')\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        h = w = self.dimension\n",
    "        N=h*w\n",
    "        # f layer\n",
    "        functionF = self.f_(x)\n",
    "        functionF = K.reshape(functionF, (-1, N, self.output_dim//4))\n",
    "        # g layer\n",
    "        functionG = self.g_(x)\n",
    "        functionG = K.reshape(functionG, (-1, N, self.output_dim//4))\n",
    "        # creating attention map\n",
    "        self.attentionMap = tf.matmul(functionF,functionG, transpose_b=True)\n",
    "        self.attentionMap = Activation('softmax', name='attnMap')(self.attentionMap)\n",
    "        # h layer\n",
    "        functionH = self.h_(x)\n",
    "        functionH = K.reshape(functionH, (-1, N, self.output_dim//2))\n",
    "        # self attention map\n",
    "        selfAttention = tf.matmul(self.attentionMap, functionH)\n",
    "        selfAttention = K.reshape(selfAttention, (-1, h, w, self.output_dim//2))\n",
    "        # v layer\n",
    "        functionV = self.v_(selfAttention)\n",
    "        return x + self.gamma*functionV\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'output_dim': self.output_dim,\n",
    "          'dimension': self.dimension,\n",
    "        }\n",
    "        base_config = super(SelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for creating Discriminator and Generator Layers\n",
    "def createDiscriminatorBlock(numberOfFilters, input, name, downsample=True):\n",
    "    residueLayer = input\n",
    "    inputChannels = input.get_shape().as_list()[-1]\n",
    "    firstLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(input)\n",
    "    firstLayer = LeakyReLU(alpha=0.2)(firstLayer)\n",
    "    secondLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(firstLayer)\n",
    "    thirdLayer = LeakyReLU(alpha=0.2)(secondLayer)\n",
    "    fourthLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(thirdLayer)\n",
    "\n",
    "    if downsample:\n",
    "        fourthLayer = AveragePooling2D(name = 'averagePoolingLayer1'+name)(fourthLayer)\n",
    "\n",
    "    if downsample or numberOfFilters != inputChannels:\n",
    "        residueLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(residueLayer)\n",
    "    if downsample:\n",
    "        residueLayer = AveragePooling2D(name = 'averagePoolingLayerResidue1'+name)(residueLayer)\n",
    "    return Add()([residueLayer, fourthLayer])\n",
    "\n",
    "def createFirstDiscriminatorBlock(numberOfFilters, input, name, downsample=True):\n",
    "    residueLayer = input\n",
    "    firstLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(input)\n",
    "    secondLayer = LeakyReLU(alpha=0.2)(firstLayer)\n",
    "    thirdLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(secondLayer)\n",
    "    thirdLayer = AveragePooling2D(name = 'averagePoolingLayer1'+name)(thirdLayer)\n",
    "    residueLayer = AveragePooling2D(name = 'averagePoolingLayerResidue1'+name)(residueLayer)\n",
    "    residueLayer = ConvSN2D(numberOfFilters,3, strides=1,padding='same')(residueLayer)\n",
    "    return Add()([thirdLayer, residueLayer])\n",
    "\n",
    "def createGeneratorBlock(numberOfFilters, input, is_training=True):\n",
    "    residueLayer = input\n",
    "    firstLayer = BatchNormalization()(input)\n",
    "    firstLayer = Activation('relu')(firstLayer)\n",
    "    secondLayer = ConvSN2DTranspose(numberOfFilters, 3, strides=2,padding='same')(firstLayer)\n",
    "\n",
    "    secondLayer = BatchNormalization()(secondLayer)\n",
    "    secondLayer = Activation('relu')(secondLayer)\n",
    "    thirdLayer = ConvSN2DTranspose(numberOfFilters, 3, strides=2,padding='same')(firstLayer)\n",
    "\n",
    "    residueLayer = ConvSN2DTranspose(numberOfFilters, 3, strides=2,padding='same')(residueLayer)\n",
    "    return Add()([residueLayer, thirdLayer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator and Generator Methods\n",
    "def discriminator():\n",
    "    segmentedImage = Input((256, 256, 1))\n",
    "    firstLayer = createFirstDiscriminatorBlock(32, segmentedImage, 'discriminatorLayer1') # 128x128\n",
    "    secondLayer = createDiscriminatorBlock(64, firstLayer, 'discriminatorLayer2') # 64x64\n",
    "    # self attention layer\n",
    "    selfAttentionLayerDown = SelfAttention(64, 64, name='selfAttentionLayerDown')(secondLayer)\n",
    "    thirdLayer = createDiscriminatorBlock(128, selfAttentionLayerDown, 'discriminatorLayer3') # 32x32\n",
    "    fourthLayer = createDiscriminatorBlock(256, thirdLayer, 'discriminatorLayer4') # 16x16\n",
    "    fifthLayer = createDiscriminatorBlock(256, fourthLayer, 'discriminatorLayer5') # 8x8\n",
    "    sixthLayer = createDiscriminatorBlock(512, fifthLayer, 'discriminatorLayer6') # 4x4\n",
    "    seventhLayer = createDiscriminatorBlock(512, sixthLayer, 'discriminatorLayer7', False) # 4x4\n",
    "    seventhLayer = LeakyReLU(alpha=0.2)(seventhLayer)\n",
    "\n",
    "    outputLayer = Flatten()(seventhLayer)\n",
    "    outputLayer = Dense(1, activation='sigmoid', name='discriminatorDenseLayer8')(outputLayer)\n",
    "    return Model(input = segmentedImage, output=outputLayer)\n",
    "\n",
    "def generator(numOfClasses):\n",
    "    colorImage = Input((256,256,3))\n",
    "    # downsample\n",
    "    firstLayer = createFirstDiscriminatorBlock(32, colorImage, 'generatorLayer1') # 128x128\n",
    "    secondLayer = createDiscriminatorBlock(64, firstLayer, 'generatorLayer2') # 64x64\n",
    "    # self attention layer\n",
    "    selfAttentionLayerDown = SelfAttention(64, 64, name='selfAttentionLayerDown')(secondLayer)\n",
    "    thirdLayer = createDiscriminatorBlock(128, selfAttentionLayerDown, 'generatorLayer3') # 32x32\n",
    "    fourthLayer = createDiscriminatorBlock(256, thirdLayer, 'generatorLayer4') # 16x16\n",
    "    fifthLayer = createDiscriminatorBlock(256, fourthLayer, 'generatorLayer5') # 8x8\n",
    "    sixthLayer = createDiscriminatorBlock(512, fifthLayer, 'generatorLayer6') # 4x4\n",
    "    seventhLayer = createDiscriminatorBlock(512, sixthLayer, 'generatorLayer7', False) # 4x4\n",
    "    seventhLayer = LeakyReLU(alpha=0.2)(seventhLayer)\n",
    "\n",
    "    # upsample\n",
    "    eighthLayer = createGeneratorBlock(256, seventhLayer) # 8x8\n",
    "    eighthLayer = Concatenate(name='layer8_concat')([eighthLayer, fifthLayer])\n",
    "\n",
    "    ninthLayer = createGeneratorBlock(256, eighthLayer) # 16x16\n",
    "    ninthLayer = Concatenate(name='layer9_concat')([ninthLayer, fourthLayer])\n",
    "\n",
    "    tenthLayer = createGeneratorBlock(128, ninthLayer) # 32x32\n",
    "    tenthLayer = Concatenate(name='layer10_concat')([tenthLayer, thirdLayer])\n",
    "\n",
    "    eleventhLayer = createGeneratorBlock(64, tenthLayer) # 64x64\n",
    "    eleventhLayer = Concatenate(name='layer11_concat')([eleventhLayer, secondLayer])\n",
    "    # self attention layer\n",
    "    selfAttentionLayerUp = SelfAttention(128, 64, name='selfAttentionLayerUp')(eleventhLayer)\n",
    "\n",
    "    twelfthLayer = createGeneratorBlock(32, selfAttentionLayerUp) # 128x128\n",
    "    twelfthLayer = Concatenate(name='layer12_concat')([twelfthLayer, firstLayer])\n",
    "    thirteenthLayer = createGeneratorBlock(32, twelfthLayer) # 256x256\n",
    "\n",
    "    output = ConvSN2D(numOfClasses,3,padding='same', activation='softmax')(thirteenthLayer)\n",
    "    return Model(colorImage, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"di...)`\n"
     ]
    }
   ],
   "source": [
    "# Creating generator and discriminator objects\n",
    "numOfClasses = 4\n",
    "generatorSAGAN = generator(numOfClasses)\n",
    "discriminatorSAGAN = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_1 (ConvSN2D)         (None, 256, 256, 32) 928         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 256, 256, 32) 0           conv_s_n2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_2 (ConvSN2D)         (None, 256, 256, 32) 9280        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 128, 128, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 128, 128, 32) 0           conv_s_n2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_3 (ConvSN2D)         (None, 128, 128, 32) 928         averagePoolingLayerResidue1genera\n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 32) 0           averagePoolingLayer1generatorLaye\n",
      "                                                                 conv_s_n2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_4 (ConvSN2D)         (None, 128, 128, 64) 18560       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 64) 0           conv_s_n2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_5 (ConvSN2D)         (None, 128, 128, 64) 36992       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128, 128, 64) 0           conv_s_n2d_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_7 (ConvSN2D)         (None, 128, 128, 64) 18560       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_6 (ConvSN2D)         (None, 128, 128, 64) 36992       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 64, 64, 64)   0           conv_s_n2d_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 64, 64, 64)   0           conv_s_n2d_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 64)   0           averagePoolingLayerResidue1genera\n",
      "                                                                 averagePoolingLayer1generatorLaye\n",
      "__________________________________________________________________________________________________\n",
      "selfAttentionLayerDown (SelfAtt (None, 64, 64, 64)   1           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_8 (ConvSN2D)         (None, 64, 64, 128)  73984       selfAttentionLayerDown[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 64, 64, 128)  0           conv_s_n2d_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_9 (ConvSN2D)         (None, 64, 64, 128)  147712      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64, 64, 128)  0           conv_s_n2d_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_11 (ConvSN2D)        (None, 64, 64, 128)  73984       selfAttentionLayerDown[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_10 (ConvSN2D)        (None, 64, 64, 128)  147712      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 32, 32, 128)  0           conv_s_n2d_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 32, 32, 128)  0           conv_s_n2d_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           averagePoolingLayerResidue1genera\n",
      "                                                                 averagePoolingLayer1generatorLaye\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_12 (ConvSN2D)        (None, 32, 32, 256)  295424      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 256)  0           conv_s_n2d_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_13 (ConvSN2D)        (None, 32, 32, 256)  590336      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 256)  0           conv_s_n2d_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_15 (ConvSN2D)        (None, 32, 32, 256)  295424      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_14 (ConvSN2D)        (None, 32, 32, 256)  590336      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 16, 16, 256)  0           conv_s_n2d_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 16, 16, 256)  0           conv_s_n2d_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 256)  0           averagePoolingLayerResidue1genera\n",
      "                                                                 averagePoolingLayer1generatorLaye\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_16 (ConvSN2D)        (None, 16, 16, 256)  590336      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 256)  0           conv_s_n2d_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_17 (ConvSN2D)        (None, 16, 16, 256)  590336      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 256)  0           conv_s_n2d_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_19 (ConvSN2D)        (None, 16, 16, 256)  590336      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_18 (ConvSN2D)        (None, 16, 16, 256)  590336      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 8, 8, 256)    0           conv_s_n2d_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 8, 8, 256)    0           conv_s_n2d_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 256)    0           averagePoolingLayerResidue1genera\n",
      "                                                                 averagePoolingLayer1generatorLaye\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_20 (ConvSN2D)        (None, 8, 8, 512)    1180672     add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 512)    0           conv_s_n2d_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_21 (ConvSN2D)        (None, 8, 8, 512)    2360320     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 512)    0           conv_s_n2d_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_23 (ConvSN2D)        (None, 8, 8, 512)    1180672     add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_22 (ConvSN2D)        (None, 8, 8, 512)    2360320     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1gene (None, 4, 4, 512)    0           conv_s_n2d_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1generatorLa (None, 4, 4, 512)    0           conv_s_n2d_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 512)    0           averagePoolingLayerResidue1genera\n",
      "                                                                 averagePoolingLayer1generatorLaye\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_24 (ConvSN2D)        (None, 4, 4, 512)    2360320     add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 4, 4, 512)    0           conv_s_n2d_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_25 (ConvSN2D)        (None, 4, 4, 512)    2360320     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 4, 4, 512)    0           conv_s_n2d_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_26 (ConvSN2D)        (None, 4, 4, 512)    2360320     leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 512)    0           add_6[0][0]                      \n",
      "                                                                 conv_s_n2d_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 4, 4, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4, 4, 512)    2048        leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 4, 4, 512)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_3 (ConvSN2 (None, 8, 8, 256)    1180416     leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_2 (ConvSN2 (None, 8, 8, 256)    1180416     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 256)    0           conv_s_n2d_transpose_3[0][0]     \n",
      "                                                                 conv_s_n2d_transpose_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer8_concat (Concatenate)     (None, 8, 8, 512)    0           add_8[0][0]                      \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 512)    2048        layer8_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 512)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_6 (ConvSN2 (None, 16, 16, 256)  1180416     layer8_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_5 (ConvSN2 (None, 16, 16, 256)  1180416     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 256)  0           conv_s_n2d_transpose_6[0][0]     \n",
      "                                                                 conv_s_n2d_transpose_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer9_concat (Concatenate)     (None, 16, 16, 512)  0           add_9[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 512)  2048        layer9_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 512)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_9 (ConvSN2 (None, 32, 32, 128)  590464      layer9_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_8 (ConvSN2 (None, 32, 32, 128)  590464      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 32, 128)  0           conv_s_n2d_transpose_9[0][0]     \n",
      "                                                                 conv_s_n2d_transpose_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer10_concat (Concatenate)    (None, 32, 32, 256)  0           add_10[0][0]                     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 256)  1024        layer10_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_12 (ConvSN (None, 64, 64, 64)   147776      layer10_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_11 (ConvSN (None, 64, 64, 64)   147776      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 64, 64, 64)   0           conv_s_n2d_transpose_12[0][0]    \n",
      "                                                                 conv_s_n2d_transpose_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer11_concat (Concatenate)    (None, 64, 64, 128)  0           add_11[0][0]                     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "selfAttentionLayerUp (SelfAtten (None, 64, 64, 128)  1           layer11_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         selfAttentionLayerUp[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_15 (ConvSN (None, 128, 128, 32) 37024       selfAttentionLayerUp[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_14 (ConvSN (None, 128, 128, 32) 37024       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 128, 128, 32) 0           conv_s_n2d_transpose_15[0][0]    \n",
      "                                                                 conv_s_n2d_transpose_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer12_concat (Concatenate)    (None, 128, 128, 64) 0           add_12[0][0]                     \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128, 128, 64) 256         layer12_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 128, 64) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_18 (ConvSN (None, 256, 256, 32) 18528       layer12_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_transpose_17 (ConvSN (None, 256, 256, 32) 18528       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256, 256, 32) 0           conv_s_n2d_transpose_18[0][0]    \n",
      "                                                                 conv_s_n2d_transpose_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_27 (ConvSN2D)        (None, 256, 256, 4)  1160        add_13[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 25,179,786\n",
      "Trainable params: 25,165,350\n",
      "Non-trainable params: 14,436\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_28 (ConvSN2D)        (None, 256, 256, 32) 352         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 256, 256, 32) 0           conv_s_n2d_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_29 (ConvSN2D)        (None, 256, 256, 32) 9280        leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 128, 128, 1)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 128, 128, 32) 0           conv_s_n2d_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_30 (ConvSN2D)        (None, 128, 128, 32) 352         averagePoolingLayerResidue1discri\n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 128, 128, 32) 0           averagePoolingLayer1discriminator\n",
      "                                                                 conv_s_n2d_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_31 (ConvSN2D)        (None, 128, 128, 64) 18560       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128, 128, 64) 0           conv_s_n2d_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_32 (ConvSN2D)        (None, 128, 128, 64) 36992       leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 128, 128, 64) 0           conv_s_n2d_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_34 (ConvSN2D)        (None, 128, 128, 64) 18560       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_33 (ConvSN2D)        (None, 128, 128, 64) 36992       leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 64, 64, 64)   0           conv_s_n2d_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 64, 64, 64)   0           conv_s_n2d_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 64, 64, 64)   0           averagePoolingLayerResidue1discri\n",
      "                                                                 averagePoolingLayer1discriminator\n",
      "__________________________________________________________________________________________________\n",
      "selfAttentionLayerDown (SelfAtt (None, 64, 64, 64)   1           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_35 (ConvSN2D)        (None, 64, 64, 128)  73984       selfAttentionLayerDown[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 64, 64, 128)  0           conv_s_n2d_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_36 (ConvSN2D)        (None, 64, 64, 128)  147712      leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 64, 64, 128)  0           conv_s_n2d_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_38 (ConvSN2D)        (None, 64, 64, 128)  73984       selfAttentionLayerDown[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_37 (ConvSN2D)        (None, 64, 64, 128)  147712      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 32, 32, 128)  0           conv_s_n2d_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 32, 32, 128)  0           conv_s_n2d_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 32, 128)  0           averagePoolingLayerResidue1discri\n",
      "                                                                 averagePoolingLayer1discriminator\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_39 (ConvSN2D)        (None, 32, 32, 256)  295424      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 32, 32, 256)  0           conv_s_n2d_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_40 (ConvSN2D)        (None, 32, 32, 256)  590336      leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 32, 32, 256)  0           conv_s_n2d_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_42 (ConvSN2D)        (None, 32, 32, 256)  295424      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_41 (ConvSN2D)        (None, 32, 32, 256)  590336      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 16, 16, 256)  0           conv_s_n2d_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 16, 16, 256)  0           conv_s_n2d_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 16, 16, 256)  0           averagePoolingLayerResidue1discri\n",
      "                                                                 averagePoolingLayer1discriminator\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_43 (ConvSN2D)        (None, 16, 16, 256)  590336      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 16, 16, 256)  0           conv_s_n2d_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_44 (ConvSN2D)        (None, 16, 16, 256)  590336      leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 16, 16, 256)  0           conv_s_n2d_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_46 (ConvSN2D)        (None, 16, 16, 256)  590336      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_45 (ConvSN2D)        (None, 16, 16, 256)  590336      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 8, 8, 256)    0           conv_s_n2d_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 8, 8, 256)    0           conv_s_n2d_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 256)    0           averagePoolingLayerResidue1discri\n",
      "                                                                 averagePoolingLayer1discriminator\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_47 (ConvSN2D)        (None, 8, 8, 512)    1180672     add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 8, 8, 512)    0           conv_s_n2d_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_48 (ConvSN2D)        (None, 8, 8, 512)    2360320     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 8, 8, 512)    0           conv_s_n2d_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_50 (ConvSN2D)        (None, 8, 8, 512)    1180672     add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_49 (ConvSN2D)        (None, 8, 8, 512)    2360320     leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayerResidue1disc (None, 4, 4, 512)    0           conv_s_n2d_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "averagePoolingLayer1discriminat (None, 4, 4, 512)    0           conv_s_n2d_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 4, 4, 512)    0           averagePoolingLayerResidue1discri\n",
      "                                                                 averagePoolingLayer1discriminator\n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_51 (ConvSN2D)        (None, 4, 4, 512)    2360320     add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 4, 4, 512)    0           conv_s_n2d_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_52 (ConvSN2D)        (None, 4, 4, 512)    2360320     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 4, 4, 512)    0           conv_s_n2d_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_s_n2d_53 (ConvSN2D)        (None, 4, 4, 512)    2360320     leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 4, 4, 512)    0           add_19[0][0]                     \n",
      "                                                                 conv_s_n2d_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 4, 4, 512)    0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "discriminatorDenseLayer8 (Dense (None, 1)            8193        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,868,482\n",
      "Trainable params: 18,861,986\n",
      "Non-trainable params: 6,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling the discriminator\n",
    "generatorSAGAN.summary()\n",
    "discriminatorSAGAN.summary()\n",
    "discriminatorSAGAN.compile(loss='binary_crossentropy', optimizer=Adam(6e-6, beta_1=0.5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGAN model method\n",
    "def createSAGan(generatorSAGAN, discriminatorSAGAN):\n",
    "    colorImage = Input((256,256,3))\n",
    "    generatorImage = generatorSAGAN(colorImage)\n",
    "    generatorImage1 =  Lambda(lambda x: K.argmax(x, axis= 3), trainable=False, name='SAGANargmax')(generatorImage)\n",
    "    generatorImage1 =  Lambda(lambda x: K.cast(x, dtype='float32'), trainable=False, name='SAGANcast')(generatorImage1)\n",
    "    generatorImage1 =  Lambda(lambda x: K.expand_dims(x, axis= 3), trainable=False, name='SAGANexpanddims')(generatorImage1)\n",
    "    discriminatorSAGAN.trainable = False\n",
    "    discriminatorOutput = discriminatorSAGAN(generatorImage1)\n",
    "    return Model(colorImage,outputs=[discriminatorOutput, generatorImage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\KerasGPU\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# loading the models\n",
    "discriminatorSAGAN = load_model('SAGanDisc_model.h5',custom_objects={'SelfAttention': SelfAttention })\n",
    "generatorSAGAN = load_model('SAGanGen_model.h5',custom_objects={'SelfAttention': SelfAttention })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 256, 256, 4)       25179786  \n",
      "_________________________________________________________________\n",
      "SAGANargmax (Lambda)         (None, 256, 256)          0         \n",
      "_________________________________________________________________\n",
      "SAGANcast (Lambda)           (None, 256, 256)          0         \n",
      "_________________________________________________________________\n",
      "SAGANexpanddims (Lambda)     (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 18868482  \n",
      "=================================================================\n",
      "Total params: 44,048,268\n",
      "Trainable params: 25,165,350\n",
      "Non-trainable params: 18,882,918\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating combined SAGAN object\n",
    "combined_model = createSAGan(generatorSAGAN, discriminatorSAGAN)\n",
    "combined_model.summary()\n",
    "combined_model.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=Adam(2e-4, beta_1=0.5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment Images by using transformations\n",
    "# https://towardsdatascience.com/data-augmentation-for-deep-learning-4fe21d1a4eb9\n",
    "# https://imgaug.readthedocs.io/en/latest/\n",
    "ia.seed(1)\n",
    "\n",
    "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n",
    "# image.\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "# Define our sequence of augmentation steps that will be applied to every image.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        #\n",
    "        # Apply the following augmenters to most images.\n",
    "        #\n",
    "        iaa.Fliplr(1), # horizontally flip all images\n",
    "        iaa.Flipud(1), # vertically flip all images\n",
    "\n",
    "        # Apply affine transformations to some of the images\n",
    "        # - scale to 80-120% of image height/width (each axis independently)\n",
    "        # - translate by -20 to +20 relative to height/width (per axis)\n",
    "        # - rotate by -45 to +45 degrees\n",
    "        # - shear by -16 to +16 degrees\n",
    "        # - order: use nearest neighbour or bilinear interpolation (fast)\n",
    "        # - mode: use any available mode to fill newly created pixels\n",
    "        #         see API or scikit-image for which modes are available\n",
    "        # - cval: if the mode is constant, then use a random brightness\n",
    "        #         for the newly created pixels (e.g. sometimes black,\n",
    "        #         sometimes white)\n",
    "        sometimes(iaa.Affine(\n",
    "            rotate=(-45, 45),\n",
    "            # shear=(-16, 16),\n",
    "            order=[0, 1],\n",
    "            cval=(0, 255),\n",
    "            mode=ia.ALL\n",
    "        )),\n",
    "    ],\n",
    "    # do all of the above augmentations in random order\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to fetch images\n",
    "def getImageNames(filePath):\n",
    "    imageNames = []\n",
    "    for fileName in os.listdir(filePath):\n",
    "        imageNames.append(fileName)\n",
    "    return imageNames\n",
    "  \n",
    "def data_generator(imageNames, batchSize):\n",
    "    imagePath = \"/content/prodocyteData/TRAINING_data/images/\"\n",
    "    annotatedImagePath = \"/content/prodocyteData/TRAINING_data/masks/\"\n",
    "    oneFourth = int(batchSize/4)\n",
    "    while True:\n",
    "        wsiImages = []\n",
    "        annotatedImages = []\n",
    "        imageBatch = np.random.choice(imageNames, 2*batchSize)\n",
    "        i = 0\n",
    "        for imageName in imageBatch:\n",
    "            # print(imageName)\n",
    "            annotatedImage = np.array(Image.open(annotatedImagePath+imageName[:-3]+'png').resize((256,256))).reshape(256,256,1)\n",
    "            if {85, 170, 255}.intersection(np.unique(annotatedImage)):\n",
    "                annotatedImage = np.array(annotatedImage/np.unique(annotatedImage)[1],dtype=np.int32)\n",
    "            wsiImages.append(np.array(Image.open(imagePath+imageName).resize((256,256)), 'float32')/127.5 -1)\n",
    "            annotatedImages.append(to_categorical(annotatedImage, num_classes=numOfClasses))\n",
    "            i += 1\n",
    "            if i == oneFourth:\n",
    "                for _ in range(2):\n",
    "                    segmentationMaps = []\n",
    "                    for segmentedImage in annotatedImages:\n",
    "                        # Convert mask to binary map\n",
    "                        np_mask = np.array(segmentedImage)\n",
    "                        np_mask = np.clip(np_mask, 0, 1)\n",
    "                        \n",
    "                        # Create segmentation map\n",
    "                        segmap = np.zeros(annotatedImages[0].shape, dtype=bool)\n",
    "                        segmap[:] = np_mask\n",
    "                        segmap = SegmentationMapOnImage(segmap, shape=annotatedImages[0].shape)\n",
    "                        segmentationMaps.append(segmap)\n",
    "                    images_aug, seg_aug = seq(images=wsiImages,segmentation_maps=segmentationMaps)\n",
    "                    for segImage in seg_aug:\n",
    "                        annotatedImages.append(segImage.arr)\n",
    "                        wsiImages.extend(images_aug)\n",
    "                break\n",
    "        yield np.array(wsiImages), np.array(annotatedImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and view a couple of pairs of original as well as segmented image\n",
    "imageNames = getImageNames(\"/content/prodocyteData/TRAINING_data/images/\")\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(imageNames)\n",
    "trainImages, testImages =  imageNames[:-100], imageNames[-100:]\n",
    "\n",
    "_, axs = plt.subplots(2, 4, figsize=(10, 10))\n",
    "for i, d in enumerate(data_generator(trainImages, 4)):\n",
    "    axs[i][0].imshow(d[0][0]*0.5+0.5)\n",
    "    axs[i][1].imshow(np.argmax(d[1], axis = -1)[0], cmap='Greys_r')\n",
    "    axs[i][2].imshow(d[0][1]*0.5+0.5)\n",
    "    axs[i][3].imshow(np.argmax(d[1], axis = -1)[1], cmap='Greys_r')\n",
    "    if(i == 1):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "%%time\n",
    "# Training SAGAN\n",
    "batchSize = 8\n",
    "iteration = 512\n",
    "epochs = 10\n",
    "discriminatorLossArray, SAGanGeneratorLoss, discriminatorAccuracy, SAGanGeneratorAccuracy = [], [], [], []\n",
    "\n",
    "real = np.ones((batchSize, 1))\n",
    "fake = np.zeros((batchSize, 1))\n",
    "for epoch in range(epochs):\n",
    "    for i, d in enumerate(data_generator(trainImages, batchSize)):\n",
    "        generatedImage = generatorSAGAN.predict(d[0])\n",
    "        generatedImage = np.argmax(generatedImage, axis=-1).reshape((batchSize,256,256,1))\n",
    "        segmentedImage =  np.argmax(d[1], axis=-1).reshape((batchSize,256,256,1))\n",
    "        discriminatorLossReal = discriminatorSAGAN.train_on_batch(segmentedImage, real)\n",
    "        discriminatorLossFake = discriminatorSAGAN.train_on_batch(generatedImage, fake)\n",
    "        discriminatorLoss = 0.5* np.add(discriminatorLossReal , discriminatorLossFake)\n",
    "        discriminatorLossArray.append(discriminatorLoss[0])\n",
    "        # discriminatorAccuracy.append(100*discriminatorLoss[1])\n",
    "        \n",
    "        SAGan_loss = combined_model.train_on_batch(d[0], [real,d[1]])\n",
    "\n",
    "        SAGanGeneratorLoss.append(SAGan_loss[0])\n",
    "        # SAGanGeneratorAccuracy.append(100*SAGan_loss[1])\n",
    "        if(i%10 == 0):\n",
    "            print(\"Epoch %d/%d   iteration %d/%d  D Acc %3d%%  D Loss: %f    SAGAN Loss: %f\" % \n",
    "                  (epoch+1, epochs, i, iteration, 100*discriminatorLoss[1], discriminatorLoss[0], SAGan_loss[0]))\n",
    "            \n",
    "        if(i == iteration-1):\n",
    "            break\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        # Writing to results to a file after 2 epochs\n",
    "        with open('drive/My Drive/IS/discriminatorLossFile.csv', mode='a') as discriminatorLossFile:\n",
    "            discriminatorLossFile_writer = csv.writer(discriminatorLossFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            discriminatorLossFile_writer.writerow(discriminatorLossArray)\n",
    "            discriminatorLossArray = []\n",
    "        with open('drive/My Drive/IS/SAGanLossFile.csv', mode='a') as SAGanLossFile:\n",
    "            SAGanLossFile_writer = csv.writer(SAGanLossFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            SAGanLossFile_writer.writerow(SAGanGeneratorLoss)\n",
    "            SAGanGeneratorLoss = []\n",
    "        rpath = 'drive/My Drive/IS/results/'\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 15))\n",
    "        for k, l in enumerate(data_generator(trainImages, 4)):\n",
    "            generatedImage = generatorSAGAN.predict(l[0])\n",
    "            axs[0].imshow(l[0][0]*0.5+0.5)\n",
    "            axs[1].imshow((np.argmax(l[1], axis=-1)[0].reshape((1,256,256,1))*0.5 + 0.5).reshape(256, 256))\n",
    "            axs[2].imshow((np.argmax(generatedImage[0], axis=-1).reshape((1,256,256,1))*0.5 + 0.5).reshape(256, 256))\n",
    "            if k == 1:\n",
    "                break\n",
    "        fig.savefig(rpath+ '{}_{}_result_{}.png'.format(epoch,i,k))\n",
    "        plt.close()\n",
    "        generatorSAGAN.save('drive/My Drive/IS/SAGanGen_model.h5')\n",
    "        discriminatorSAGAN.save('drive/My Drive/IS/SAGanDisc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to show graphs\n",
    "# https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar\n",
    "def smooth(scalars, weight) :  # Weight between 0 and 1\n",
    "    if not scalars: return\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "    return smoothed\n",
    "  \n",
    "# https://realpython.com/python-csv/\n",
    "def plotGraph(filename, smoothingFactor):\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        graph = []\n",
    "        for row in csv_reader:\n",
    "            for column in row:\n",
    "                graph.append(float(column))\n",
    "    graph = smooth(graph, smoothingFactor)\n",
    "    plt.plot(graph)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discriminator Loss\n",
    "plotGraph('discriminatorLossFile.csv', 0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Metrics to show results\n",
    "def conf_matrix(y_true, y_pred, labels):\n",
    "    conf_mat = np.zeros((labels, labels))\n",
    "    for b in range(y_true.shape[0]):\n",
    "        for i in range(y_true.shape[1]):\n",
    "            for j in range(y_true.shape[2]):\n",
    "                conf_mat[y_true[b][i][j]][y_pred[b][i][j]] += 1\n",
    "    return np.array(conf_mat)\n",
    "\n",
    "def accuracy(cm):\n",
    "    return np.sum(np.diag(cm))/cm.sum()\n",
    "\n",
    "def recall(cm, labels):\n",
    "    r = np.diag(cm)/np.sum(cm, axis = 1)\n",
    "    return np.mean(r), np.rec.fromarrays([labels.reshape(-1, 1), r.reshape(-1,1)], names=['label', 'value'])\n",
    "\n",
    "def precision(cm, labels):\n",
    "    p = np.diag(cm)/np.sum(cm, axis = 0)\n",
    "    return np.mean(p), np.rec.fromarrays([labels.reshape(-1, 1), p.reshape(-1,1)], names=['label', 'value'])\n",
    "\n",
    "def mIOU(cm, labels):\n",
    "    iou = np.diag(cm)/(np.sum(cm, axis=0) + np.sum(cm, axis=1) - np.diag(cm))\n",
    "    return np.mean(iou), np.rec.fromarrays([labels.reshape(-1, 1), iou.reshape(-1,1)], names=['label', 'value'])\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    return (2*precision*recall)/(precision+recall)\n",
    "\n",
    "def get_all_metrics(generator, labels):\n",
    "    annotatedImages = []\n",
    "    segmentedImages = []\n",
    "    for image in testImages:\n",
    "        imagePath = \"/content/prodocyteData/TRAINING_data/images/\"\n",
    "        annotatedImagePath = \"/content/prodocyteData/TRAINING_data/masks/\"\n",
    "        annotatedImage = np.array(Image.open(annotatedImagePath+image[:-3]+'png').resize((256,256))).reshape(256,256)\n",
    "        if {85, 170, 255}.intersection(np.unique(annotatedImage)):\n",
    "            annotatedImage = np.array(annotatedImage/np.unique(annotatedImage)[1],dtype=np.int32)\n",
    "        annotatedImages.append(annotatedImage)\n",
    "        generatedImage = generator.predict(np.array(np.array(Image.open(imagePath+image).resize((256,256)), 'float32')/127.5 -1).reshape(1,256,256,3))\n",
    "        segmentedImage = (np.argmax(generatedImage, axis=-1)[0].reshape((1,256,256,1))).reshape(256, 256)\n",
    "        segmentedImages.append(segmentedImage)\n",
    "    dic = {}\n",
    "    cm = conf_matrix(np.array(annotatedImages), np.array(segmentedImages), len(labels))\n",
    "    dic['full_accuracy'] = accuracy(cm)*100\n",
    "    dic['class_accuracy'] = accuracy(cm[1:, 1:])*100\n",
    "    dic['precision'], dic['per_class_precision'] = precision(cm, labels)\n",
    "    dic['recall'], dic['per_class_recall'] = recall(cm, labels)\n",
    "    dic['mIOU'], dic['IOU'] = mIOU(cm, labels)\n",
    "    dic['f1_score'] = f1_score(dic['precision'], dic['recall'])\n",
    "    dic['per_class_f1_score'] = f1_score(dic['per_class_precision']['value'], dic['per_class_recall']['value'])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictResults = get_all_metrics(generatorSAGAN, np.array(['background', 'glomeruli', 'podocyte', 'non-podocyte']))\n",
    "print(dictResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of segmentation on one image\n",
    "from sklearn.metrics import f1_score\n",
    "_, axs = plt.subplots(2, 3, figsize=(10, 10))\n",
    "for i, d in enumerate(data_generator(testImages, 4)):\n",
    "    axs[i][0].imshow(d[0][0]*0.5+0.5)\n",
    "    generatedImage = generatorSAGAN.predict(d[0])\n",
    "    segmentedImage = (np.argmax(generatedImage, axis=-1)[0].reshape((1,256,256,1))).reshape(256, 256)\n",
    "    axs[i][2].imshow(segmentedImage)\n",
    "    print('predict', np.unique(segmentedImage,return_counts=True))\n",
    "    actualImage = np.argmax(d[1][0],axis=-1)\n",
    "    axs[i][1].imshow(actualImage)\n",
    "    print('actual ', np.unique(actualImage,return_counts=True))\n",
    "    print('class 0 - score - ', f1_score(actualImage.ravel(), segmentedImage.ravel(), labels=[0], average='micro'))\n",
    "    print('class 1 - score - ', f1_score(actualImage.ravel(), segmentedImage.ravel(), labels=[1], average='micro'))\n",
    "    print('class 2 - score - ', f1_score(actualImage.ravel(), segmentedImage.ravel(), labels=[2], average='micro'))\n",
    "    print('class 3 - score - ', f1_score(actualImage.ravel(), segmentedImage.ravel(), labels=[3], average='micro'))\n",
    "    if(i == 1):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
